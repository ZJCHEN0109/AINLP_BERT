{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "poet_for_you_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZJCHEN0109/AINLP_BERT/blob/main/poet_for_you_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDerWlLdxH2X",
        "outputId": "6fdabfda-b598-48d8-d934-ede0448b4394"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iozNZup4wkPN"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CmAvQm3wkPZ"
      },
      "source": [
        "# 為你寫詩 (pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NkX-wJ7wkPh"
      },
      "source": [
        "支援python 版本: 3.5以上  \n",
        "支援pytorch版本 : 1.2以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_MaW6sDwkPk"
      },
      "source": [
        "詩詞歌賦是人類語言的精華，若要說什麼是語言能力最高的表現，那大概就是寫詩或是寫小說了。我們今天就要來嘗試讓機器也能夠生成詩詞歌賦，不過大家不要以為AI已經攻破了這個任務，相反的，人工智能在語言這條路上還有許多路得走，今天的實作機器所寫出來的與其說是創作，倒不如說是鸚鵡學舌來的貼切，機器的確能在大量語料中學習到語言的部分語義，搭配LSTM這個專門用於序列數據的算法，還是能夠產生一些蠻有趣的內容。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J0n_gBXwkPl"
      },
      "source": [
        "![Alt text](../images/tang-shi.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tridentx --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcptYlnYzChH",
        "outputId": "99383f9d-ad48-4bfb-c32c-d08092f0f859"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tridentx\n",
            "  Downloading tridentx-0.7.5-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tridentx) (4.64.0)\n",
            "Requirement already satisfied: scikit-image>=0.15 in /usr/local/lib/python3.7/dist-packages (from tridentx) (0.18.3)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from tridentx) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from tridentx) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from tridentx) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tridentx) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tridentx) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from tridentx) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tridentx) (57.4.0)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from tridentx) (2.8.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tridentx) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from tridentx) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tridentx) (3.13)\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from tridentx) (0.3.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.2->tridentx) (4.1.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx) (2.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (3.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.46.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->tridentx) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->tridentx) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->tridentx) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->tridentx) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->tridentx) (1.5.2)\n",
            "Installing collected packages: tridentx\n",
            "Successfully installed tridentx-0.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv4pCZwUwkPn",
        "outputId": "bf9071c1-ee94-4c3b-a801-1df53e9a716f"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "os.environ['TRIDENT_BACKEND'] = 'pytorch'\n",
        "os.environ['TRIDENT_HOME'] = '/content/gdrive/MyDrive/trident'\n",
        "# !pip uninstall tridentx\n",
        "# !pip install '/content/gdrive/MyDrive/trident/tridentx-0.7.4-py3-none-any.whl'\n",
        "!pip install '/content/gdrive/MyDrive/DeepBelief_Course5_Examples/tridentx-0.7.4-py3-none-any.whl'\n",
        "#!pip install tridentx --upgrade\n",
        "# import trident as T\n",
        "from trident import *\n",
        "from trident.models.pytorch_densenet import DenseNetFcn\n",
        "import nltk.corpus"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./gdrive/MyDrive/DeepBelief_Course5_Examples/tridentx-0.7.4-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (3.2.2)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (2.8.0)\n",
            "Requirement already satisfied: scikit-image>=0.15 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (0.18.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (57.4.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (1.21.6)\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (0.3.5.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (1.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (4.1.2.30)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tridentx==0.7.4) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx==0.7.4) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx==0.7.4) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx==0.7.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx==0.7.4) (1.4.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.2->tridentx==0.7.4) (4.1.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx==0.7.4) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx==0.7.4) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx==0.7.4) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx==0.7.4) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (1.46.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (3.3.7)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx==0.7.4) (3.17.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx==0.7.4) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx==0.7.4) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx==0.7.4) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->tridentx==0.7.4) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->tridentx==0.7.4) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->tridentx==0.7.4) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->tridentx==0.7.4) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx==0.7.4) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx==0.7.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx==0.7.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx==0.7.4) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->tridentx==0.7.4) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->tridentx==0.7.4) (1.5.2)\n",
            "tridentx is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qHQlwNwwkPt"
      },
      "source": [
        "目前所有AI都是基於數據所驅動，因此我準備了6份語料，大家都可以套用在這次介紹的架構中進行訓練與推論，您也可以換成自己準備的語料文字檔(請注意需要有足夠數量才能夠訓練得夠好。)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S4zsNAFwkPw"
      },
      "source": [
        "item='jay' #周杰倫歌詞   \n",
        "item='lingxi' #林夕歌詞   \n",
        "item='poets54' #五言絕句   \n",
        "item='poets74' #七言絕句   \n",
        "item='poets58' #五言律詩  \n",
        "item='poets78' #七言律詩  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzyOhaqEwkPy"
      },
      "source": [
        "    \n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7iMWhnhwkP1"
      },
      "source": [
        "## 為你寫歌詞"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnWKh1eWwkP3"
      },
      "source": [
        "我們先從難度較低的開始著手，那就先用周董的歌吧。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "g83pvRXTwkP4",
        "outputId": "3ece9a16-6172-4363-ea80-4ec65fd81708"
      },
      "source": [
        "item='jay' \n",
        "data_provider,original_corpus=load_text(item+'.txt',unit='char',mode='next_word',is_onehot=False,sequence_length=8,return_corpus=True,sequence_start_at='random')\n",
        "#全形轉半形\n",
        "# data_provider.traindata.data.transform_funcs=[\n",
        "#    ToHalfWidth()\n",
        "#    ]\n",
        "t1,t2=data_provider.next()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total distinct chars: 2496\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5cc5d6fac9e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jay'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginal_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'char'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'next_word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#全形轉半形\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# data_provider.traindata.data.transform_funcs=[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    ToHalfWidth()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trident/data/data_loaders.py\u001b[0m in \u001b[0;36mload_text\u001b[0;34m(filname, data, label, unit, mode, section_delimiter, sequence_start_at, is_onehot, encoding, sequence_length, return_corpus, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcorpus2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mdata_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTextSequenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobject_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mObjectType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mlabels_seq\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mTextSequenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobject_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mObjectType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mtraindata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdataprovider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextSequenceDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trident/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, is_onehot, sequence_offset, storage_unit, section_delimiter, stopwords, sequence_length, sequence_start_at, include_segment_ids, include_mask_ids, object_type, symbol, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_paired_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_spatial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_onehot\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trident/data/dataset.py\u001b[0m in \u001b[0;36madd_corpus\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus should be a collection.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: corpus should be a collection."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(item+'.txt')"
      ],
      "metadata": {
        "id": "9F28e6Wl_h07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjkGXdeKwkP6"
      },
      "source": [
        "在準備資料方面，我所設計的trident api中其實是透過TextSequenceDataset(負責儲存語料，以及將語料轉換為序列張量)以及TextSequenceDataProvider(作為提供數據的generator，負責供應數據的接口)這兩個物件來負責的。其中只需要透過比較簡單的load_text函數就可以設計好前述兩者的設定。loadtext的設定引數如下：\n",
        "\n",
        "     filname (str)  :文字檔案來源的路徑\n",
        "     data (str,list): 輸入語料\n",
        "     label (str,list): 輸出語料\n",
        "     unit (str)     :文字分割的基礎，有效值包括'char'(字)以及'word'(詞)，預設為'char'，目前中文尚不支援自動分詞\n",
        "     mode (str)     :產生序列模式，有效值包括'next_word'(預測下一個字),skip_gram(前後文預測當前字元),cbow(當前字元預測前後文),\n",
        "                     onehot(產生onehot向量),1to1_seq2seq (一對一序列對序列)\n",
        "     section_delimiter (str): 分區的分隔符號，為了避免撞號，因此要求需要使用連續兩個字元作為分隔符號，預測為'\\n\\n'\n",
        "     sequence_start_at (str)：序列起始點選取方法，有效值包括'random'(隨機起始)，'slide'(活動窗口，意味著前後序列可能重疊)\n",
        "                     ,'follow_up'(接續，前後序列不重疊),'section_start'(以每個分段起點作為開始)\n",
        "     is_onehot(bool): 是否為onehot\n",
        "     encoding       :編碼，目前預設值為'utf-8-sig':\n",
        "     sequence_length (int) :序列長度，預設值為64\n",
        "     return_corpus(bool)   : 除了data provider外是否回傳原始語料\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0d0cw_ZwkP8"
      },
      "source": [
        "其中為何在自動寫歌詞的 **sequence_start_at** 要設定為 **random**呢?最主要是現代歌詞比較沒有固定格律，每句長短也不一，如果使用section_start，則句末會以 **[PAD]** 填滿，這在我們日後生成文字時會造成困擾(除非序列定長，否則我們無法預期PAD何時結束)，所以我們採取序列取樣方法為 **random** 這樣可以強化模型不需要句子從頭開始完整語意仍可預測序列的能力，而且設定的序列長度較短，也避免[PAD]的發生。\n",
        "\n",
        "我們將data_provider提供的數據根據它自身的index2text方法還原，各位可以看到它除了語料本身之外，還額外插入了**[CLS],[SEP],[PAD]**等標籤，以作為文字分段的基礎參考。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "AkRX7ElJwkP9",
        "outputId": "30d65420-1600-4a46-ade2-c57b7361a600"
      },
      "source": [
        "for k in range(8):\n",
        "    print(''.join([data_provider.index2text(i) if i!=3 else '' for i in t1[k]]))\n",
        "    print(''.join([data_provider.index2text(i) if i!=3 else '' for i in t2[k]]))\n",
        "    print('')\n",
        "    \n",
        "print(''.join(data_provider.vocabs))\n",
        "print(data_provider.signature)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-885627ed87b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 't1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zKHwfGPwkQA"
      },
      "source": [
        "這次要實作的模型說是機器寫作，其實本質就是要機器基於已經知道的序列內容，預測下一個字。所以這種一個字一個字生成的模式，來構築出整個內容。這個也是深度學習語言模型中最基礎的Char-rnn。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4qs_D6BwkQC"
      },
      "source": [
        "![Alt text](../images/unlimited_monkey.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6KYW5wpwkQD"
      },
      "source": [
        "數學上有個「無限猴子理論」，說是給猴子一台打字機，只要給他無限的時間，總有一天可以打出莎士比亞全集。這樣講有點作弊，看來得是要等到猴子進化成人類後才寫得出來。期待猴子實在沒可能，那我們能否寄望神經網路呢?當然可以，我們將會示範如何透過lstm來讓機器能夠吟詩作賦。以下是這次的char-cnn的結構以及每個階段的張量尺寸。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASL2cxK3wkQD"
      },
      "source": [
        "![Alt text](../images/char_rnn.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjTaVMvqwkQE"
      },
      "source": [
        "模型結構就如同前圖所介紹的，我們預計將透過是將文字張量，先通過Embedding層抽取表徵，經過正規化之後直接導入至LSTM中，輸出的結果需要reshape在經過正規化以及全連接層，就可以輸出下一個字的預測值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL_FEsuIwkQF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "2e9d28b5-48ce-4db1-9806-f913dc31ed90"
      },
      "source": [
        "h_size=512\n",
        "num_chars=len(data_provider.vocabs)\n",
        "\n",
        "\n",
        "lstm1=Sequential(\n",
        "    Embedding(embedding_dim=256,num_embeddings=num_chars,sparse=False,name='embed'),\n",
        "    BatchNorm(in_sequence=True),\n",
        "    LSTM(hidden_size=h_size,num_layers=2,stateful=False,batch_first=False,dropout_rate=0.2, name='lstm'),\n",
        "    Reshape((h_size)),\n",
        "    Dense(num_chars,use_bias=False,activation=None, name='fc'),\n",
        "    SoftMax(-1,name='softmax')\n",
        "    \n",
        ")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-dd101af865ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mh_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m lstm1=Sequential(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_provider' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO-Q4zmLwkQG"
      },
      "source": [
        "我們將模型利用前面provider供應的數據作為範例資料來指定輸入與輸出，這樣就完成了模型結構設計。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "6X2pzpTwwkQG",
        "outputId": "6a232a9a-8aa3-43d4-87e8-4159b771568a"
      },
      "source": [
        "\n",
        "lstm_jay=Model(inputs=t1,output=lstm1)\n",
        "\n",
        "\n",
        "#if os.path.exists('Models/{0}.pth.tar'.format(item)):\n",
        "#    lstm_jay.load_model('Models/{0}.pth.tar'.format(item))\n",
        "lstm_jay.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cb750aed43ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_jay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#if os.path.exists('Models/{0}.pth.tar'.format(item)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 't1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dT70V5gwkQI"
      },
      "source": [
        "在這個文字生成模型，本質上就是一個分類問題，根據前面的狀態評估下一個字是什麼，所以我們直接使用常見的CrossEntropyLoss就可以，但是別忘了，由於是在序列內，因此目標值的形狀會是 **(批次,序列長)**，這跟已經經過reshape層的輸出值不一樣，所以目標值也需要reshape一下。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Zxvc7zwkQJ"
      },
      "source": [
        "\n",
        "def SequenceCrossEntropyLoss(output, target):\n",
        "    target = target.reshape(-1)\n",
        "    mask = target != 3\n",
        "    masked_target = target[mask]\n",
        "    masked_output = output[mask, :]\n",
        "    return CrossEntropyLoss(reduction='mean', axis=-1)(masked_output, masked_target) + 0.2 * CrossEntropyLoss(reduction='mean', auto_balance=True, axis=-1)(output, target)\n",
        "\n",
        "\n",
        "def SequenceAccuracy(output, target):\n",
        "    target = target.reshape(-1)\n",
        "    mask = target != 3\n",
        "    masked_target = target[mask]\n",
        "    masked_output = output[mask, :]\n",
        "    return accuracy(masked_output, masked_target, axis=-1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcg02uDowkQK"
      },
      "source": [
        "在最後模型的設定基本上是將剛才處理過的支持序列的CrossEntropyLoss以及Accuracy分別設定為損失函數與評估函數，為了避免LSTM梯度爆炸(理論上很少發生，但是以防萬一)，我加上了**梯度裁切(gradient clipping)**，然後我希望在模型訓練過程中可以即時看到預測的效果，因此我們需要讓訓練流程中定期調用剛剛寫好的write_something函數，以前是需要透過客製化Callbacks的方式來實現，**現在在trident中有更簡單的新方法就是使用模型中的trigger_when方法**。透過trigger_when的action(對應到輸入值為training_context的函數)以及設定基於什麼來觸發action的事件，舉例來說，我希望每500個批次就調用一次write_something函數，因此就可以設定在on_batch_end事件中觸發，不過每次觸發時，可以透過檢查training_context中目前的epoch以及batch數來判斷是否要繼續執行，這樣等於不需要重新撰寫callbacks類，就能做到同樣的效果。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F41-b0VuwkQL"
      },
      "source": [
        "\n",
        "def write_something(training_context):\n",
        "    \"\"\"\n",
        "\n",
        "    :param training_context:\n",
        "    :type training_context:\n",
        "    \"\"\"\n",
        "    global data_provider, original_corpus,item\n",
        "\n",
        "    model = training_context['current_model']\n",
        "    # 模型千萬記得要轉換成eval模式\n",
        "    model.eval()\n",
        "    for  module in model.modules():\n",
        "        if isinstance(module, LSTM):\n",
        "            module.stateful=True\n",
        "\n",
        "\n",
        "    print()\n",
        "    temperatures =[0.8,1.0,1.2,1.5,1.8]\n",
        "    for k in range(5):\n",
        "        temperature=temperatures[k]\n",
        "      \n",
        "\n",
        "        model.eval()\n",
        "        #每次撰寫之一開頭先把hx清掉，並設定為stateful\n",
        "        for  module in model.modules():\n",
        "          if isinstance(module, LSTM):\n",
        "              module.hx=None\n",
        "              module.stateful=True\n",
        "\n",
        "        text_generated = []\n",
        "        start_string = random_choice(original_corpus)[:1]\n",
        "        if isinstance(start_string,list):\n",
        "            start_string=''.join(start_string)[:2]\n",
        "        text_generated.append('[CLS]')\n",
        "        text_generated.extend(list(start_string))\n",
        "\n",
        "\n",
        "        seq = [data_provider.text2index(s) for s in start_string]\n",
        "        seq.insert(0, 0)\n",
        "        # seq.append(1)\n",
        "        input_eval = to_tensor([seq]).long().detach()\n",
        "        if ndim(input_eval)<2:\n",
        "            input_eval=input_eval.expand_dims(0)\n",
        "\n",
        "        print('----- temperature:', temperature)\n",
        "        print('----- 根據以下詞彙發想:「{0}」'.format(start_string))\n",
        "        print('')\n",
        "        is_finished = False\n",
        "        num_generate = 0\n",
        "        row_length=0\n",
        "\n",
        "        sys.stdout.write(start_string)\n",
        "\n",
        "        while not is_finished:\n",
        "            try:\n",
        "\n",
        "                F\n",
        "                #將溫度設定僅限於機率最高的十個字\n",
        "                predicted_idx=argsort(predictions)[:10]\n",
        "                predicted_probs =  clip(predictions[predicted_idx],1e-8,1-1e-8)\n",
        "\n",
        "                predicted_id = predicted_idx[multinomial(predicted_probs/ temperature, num_samples=1)].item()\n",
        "                #predicted_id = argmax(predictions).item()\n",
        "\n",
        "                # We pass the predicted word as the next input to the model\n",
        "                # along with the previous hidden state\n",
        "                input_eval = to_tensor([[predicted_id]]).long().detach()\n",
        "                if ndim(input_eval) < 2:\n",
        "                    input_eval = input_eval.expand_dims(0)\n",
        "\n",
        "                text_generated.append(data_provider.index2text(predicted_id))\n",
        "\n",
        "                if len(text_generated)>15 and len(list(set(text_generated[-10:])))==1:\n",
        "                    is_finished = True\n",
        "                    break\n",
        "                if text_generated[-2] == '[SEP]' and text_generated[-1] == '[PAD]':\n",
        "                    sys.stdout.write('\\n')\n",
        "                    is_finished=True\n",
        "                    sys.stdout.flush()\n",
        "                elif  text_generated[-2] == '\\n' and text_generated[-1] == '\\n':\n",
        "                    sys.stdout.write('\\n')\n",
        "                    is_finished=True\n",
        "                    sys.stdout.flush()\n",
        "                elif text_generated[-2] == '[SEP]' and text_generated[-1]=='[CLS]':\n",
        "                    sys.stdout.write('\\n')\n",
        "                    sys.stdout.flush()\n",
        "                elif text_generated[-1] not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']:\n",
        "                    sys.stdout.write(text_generated[-1])\n",
        "                    row_length += 1\n",
        "                    if item=='poets58'  and row_length >= 47 and text_generated[-1] in ['，', '。',' ', '[SEP]']:\n",
        "                        sys.stdout.write('\\n')\n",
        "                        is_finished=True\n",
        "                    sys.stdout.flush()\n",
        "                num_generate += 1\n",
        "                if num_generate > 500:\n",
        "                    is_finished = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        print(' ')\n",
        "    model.train()\n",
        "    for  module in model.modules():\n",
        "        if isinstance(module, LSTM):\n",
        "            module.stateful=False\n",
        "            module.hx=None\n",
        "    print()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjtlY53XwkQM"
      },
      "source": [
        "既然說是文本生成模型，那當然推論階段是要能自動將文字內容產生出來，在此我們透過write_something函數，隨機從語料中抽取案例，利用他的頭幾個字作為提示來開始生成文本。在這個階段建議不要抽太多字，否則你會看到LSTM優異的...**背書能力**。由於背歌詞實在沒有什麼讓人太驚艷的，為了增加意外性，我們就不直接利用argmax找出最高機率位置的方式來生成文字，而是透過溫度的概念，**除以一個高於1的溫度值(降溫)搭配基於多項式機率分布**來抽樣，這樣其他非最高機率的字才有機會可以出線，這樣文字生成才不會變成背書大賽。而我是把傳統多項式機率分布的範圍從全部詞彙，改成機會最高的前十個字，以免傳統作法容易出現不通順的問題。\n",
        "\n",
        "此外，還需要注意的是，在訓練過程中，由於我們沒有辦法確保每個批次之間是來自於同個作品，因此會設定stateful=False(這也是預設值)，這樣每個批次執行前會清掉初始隱藏狀態與內部狀態，但是在推論階段，由於我們是一次傳一個字，因此需要連續的狀態，因此要記得推論階段要把stateful設定為True。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "AQoaM-1vwkQN",
        "outputId": "2f5e11e7-ee07-41eb-fdb5-064a263b1532"
      },
      "source": [
        "\n",
        "lstm_jay\\\n",
        "    .with_optimizer(optimizer='Adam',lr=1e-3,betas=(0.9, 0.999))\\\n",
        "    .with_loss(SequenceCrossEntropyLoss)\\\n",
        "    .with_metric(SequenceAccuracy,name='accuracy')\\\n",
        "    .with_regularizer('l2')\\\n",
        "    .with_model_save_path('Models/{0}.pth.tar'.format(item))\\\n",
        "    .trigger_when('on_batch_end', action=write_something,frequency=200,unit='batch') \\\n",
        "    .with_learning_rate_scheduler(reduce_lr_on_plateau,monitor='accuracy',mode='max',factor=0.5,patience=2,cooldown=0,threshold=2e-3,warmup=0) \\\n",
        "    .with_automatic_mixed_precision_training()\n",
        "\n",
        "    #.with_optimizer(optimizer='Adam',lr=1e-3,betas=(0.9, 0.999))\\ 設定優化器\n",
        "    #.with_loss(SequenceCrossEntropyLoss)\\ 設定損失函數\n",
        "    #.with_metric(SequenceAccuracy,name='accuracy')\\ 設定評估函數\n",
        "    #.with_regularizer('l2')\\ 設定l2regularizer\n",
        "    #.with_model_save_path('Models/{0}.pth.tar'.format(item))\\ 設定模型存檔路徑\n",
        "    #.trigger_when('on_batch_end', action=write_something) \\ #設定觸發任務write_something\n",
        "    #設定調整學習速率時程\n",
        "    #.with_learning_rate_scheduler(reduce_lr_on_plateau,monitor='accuracy',mode='max',factor=0.5,patience=3,cooldown=0,threshold=5e-4,warmup=0) \\\n",
        "    #.with_automatic_mixed_precision_training() #設定為自動混合精度訓練\n",
        "\n",
        "plan=TrainingPlan()\\\n",
        "    .add_training_item(lstm_jay)\\\n",
        "    .with_data_loader(data_provider)\\\n",
        "    .repeat_epochs(3)\\\n",
        "    .with_batch_size(64)\\\n",
        "    .print_progress_scheduling(10,unit='batch')\\\n",
        "    .display_loss_metric_curve_scheduling(frequency=200,unit='batch',imshow=True)\\\n",
        "    .print_gradients_scheduling(100)\\\n",
        "    .save_model_scheduling(100,unit='batch')\n",
        "\n",
        "    #.add_training_item(lstm_jay)\\ 加入要訓練的模型\n",
        "    #.with_data_loader(data_provider)\\ 設定資料提供者\n",
        "    #.repeat_epochs(10)\\  設定總epoch數\n",
        "    #.with_batch_size(64)\\ 設定批次大小\n",
        "    #.print_progress_scheduling(20,unit='batch')\\ 設定列印訓練狀態的時程\n",
        "    #.display_loss_metric_curve_scheduling(frequency=200,unit='batch',imshow=True)\\ 設定顯示loss-metrics時間走勢圖的時程\n",
        "    #.print_gradients_scheduling(200)\\ 設定列印梯度狀況的時程\n",
        "    #.save_model_scheduling(100,unit='batch') 設定存檔的時程\n",
        "\n",
        "plan.start_now()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2262d03ec0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_jay\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwith_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwith_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequenceCrossEntropyLoss\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwith_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequenceAccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwith_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwith_model_save_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Models/{0}.pth.tar'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mtrigger_when\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite_something\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwith_learning_rate_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_lr_on_plateau\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34...\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#.with_optimizer(optimizer='Adam',lr=1e-3,betas=(0.9, 0.999))\\ 設定優化器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#.with_loss(SequenceCrossEntropyLoss)\\ 設定損失函數\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm_jay' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pQIbNvwkQO"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FS4h4xVwkQP"
      },
      "source": [
        "## 為你寫唐詩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgWCCI1awkQP"
      },
      "source": [
        "既然是「為你寫詩」，當然除了寫歌詞之外，還要挑戰更難一點的任務，那就是寫真正的詩，而且是唐詩的風格。我這邊分別準備了五言絕句(poets54.txt，共4065首)、七言絕句(poets74.txt，共10730首)、五言律詩(poets58.txt，共14409首)、七言律詩(poets78.txt，共8180首)這四種語料，我們就來挑戰語料量最豐富的五言律詩。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "pRQyxdk0wkQQ",
        "outputId": "94456d6d-a268-4e0e-96ac-fa084cc70edd"
      },
      "source": [
        "item='poets58' \n",
        "\n",
        "#這裡sequence_start_at='section_start'主要是因為五言律詩有固定句長，因此希望模型記憶整個句長(5*8加上中間7個空白,以及cls)\n",
        "data_provider,original_corpus=load_text(item+'.txt',unit='char',is_onehot=False,sequence_length=50,sequence_start_at='section_start',return_corpus=True)\n",
        "t1,t2=data_provider.next()\n",
        "\n",
        "for k in range(8):\n",
        "    print(''.join([data_provider.index2text(i) if i!=3 else '' for i in t1[k]]))\n",
        "    print(''.join([data_provider.index2text(i) if i!=3 else '' for i in t2[k]]))\n",
        "    print('')\n",
        "    \n",
        "print(data_provider.signature)\n",
        "original_corpus=[item for item in original_corpus if len(original_corpus)>=50]\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total distinct chars: 5746\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-6033e03a5205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#這裡sequence_start_at='section_start'主要是因為五言律詩有固定句長，因此希望模型記憶整個句長(5*8加上中間7個空白,以及cls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginal_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'char'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'section_start'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trident/data/data_loaders.py\u001b[0m in \u001b[0;36mload_text\u001b[0;34m(filname, data, label, unit, mode, section_delimiter, sequence_start_at, is_onehot, encoding, sequence_length, return_corpus, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcorpus2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mdata_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTextSequenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobject_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mObjectType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mlabels_seq\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mTextSequenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_start_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobject_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mObjectType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mtraindata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdataprovider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextSequenceDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trident/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, is_onehot, sequence_offset, storage_unit, section_delimiter, stopwords, sequence_length, sequence_start_at, include_segment_ids, include_mask_ids, object_type, symbol, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_paired_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_spatial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_onehot\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trident/data/dataset.py\u001b[0m in \u001b[0;36madd_corpus\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus should be a collection.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: corpus should be a collection."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wboBsIwvwkQS"
      },
      "source": [
        "五言律詩由於有上萬首，使用的中文字數當然也不少，高達了5745個，再加上古文用字精簡，背後還有格律平仄對仗等隱規則，當然我們這次沒有提供文字以外的資訊給機器學，所以要光從文字本身找到隱含的語義其實難度頗高。但是只要是剛才介紹的模型結構以及log_softmax的收斂技巧應該只要稍微調整嵌入層特徵大小，以及增加隱藏層尺寸應該是可以勝任這個挑戰，但是未來專案中或是遇到搞不定的模型，這個時候該如何解決這個問題呢。\n",
        "\n",
        "在訓練多分類的rnn模型時，我會有以下幾個建議：\n",
        "\n",
        "**1. 不要盲目地增加lstm層數，除非你很有空閒時間很多可以看他慢慢收斂，一般建議兩層即可，再多層只是變得更難訓練。**\n",
        "\n",
        "**2. 不要以為改用雙向lstm效果會變好，因為這是預測下一個字，所以建模時事知道全句所以看似效果很好，但是實際推論時則是很糟。**\n",
        "\n",
        "**3. 在嵌入層、lstm隱藏層一直到最後全連接層的輸出，建議尺寸的設定依照金字塔型(pyrimid)的方式設定也就是這三層依照接近等比級數的方式設定是最好的，應該是前面小，後面逐步放大，所以我這邊是依序設為256,1024, 5869以接近4倍遞增的形式來設定。** \n",
        "\n",
        "**4. 除了在網絡結構上找問題，更應該先檢視語料有無問題，是否有編碼不一致造成的無法顯示字元、罕見字，全形字或者是unicode中日韓區域中非中文使用漢字，排除清理這些會讓模型訓練的更有效率**      \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "sJyH1YdHwkQT",
        "outputId": "7e8630ce-1ff8-400f-c707-4a6197d61cfe"
      },
      "source": [
        "h_size=1024\n",
        "num_chars=len(data_provider.vocabs)\n",
        "print(num_chars)\n",
        "\n",
        "#在這裡LSTM模型使用了雙向，可以讓模型更理解語意，同時將stateful開啟以記憶語意\n",
        "lstm2=Sequential(\n",
        "    Embedding(embedding_dim=256,num_embeddings=num_chars,sparse=False),\n",
        "    BatchNorm(in_sequence=True,affine=True),\n",
        "    LSTM(hidden_size=h_size,num_layers=2,stateful=False,batch_first=False,dropout_rate=0.2),\n",
        "    Reshape((h_size)),\n",
        "    Dense(num_chars,use_bias=False,activation=None, name='fc'),\n",
        "    SoftMax(-1,name='softmax')\n",
        "    )\n",
        "\n",
        "lstm_poet2=Model(inputs=t1,output=lstm2)\n",
        "lstm_poet2.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ef439f09adaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mh_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#在這裡LSTM模型使用了雙向，可以讓模型更理解語意，同時將stateful開啟以記憶語意\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_provider' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFe4P9Pv-yLn"
      },
      "source": [
        "from trident.callbacks.lr_schedulers import AdjustLRCallbackBase\n",
        "class CosineLR(AdjustLRCallbackBase):\n",
        "    def __init__(self, max_lr=None, min_lr=1e-7,period=1000,cosine_weight=0.2,**kwargs):\n",
        "        super(CosineLR, self).__init__()\n",
        "        self.max_lr = max_lr\n",
        "        self.min_lr = min_lr\n",
        "        self.period=period\n",
        "        self.T_max=period/2\n",
        "        self.T_current = 0\n",
        "        self.cosine_weight=cosine_weight\n",
        "    def on_batch_end(self, training_context):\n",
        "        if self.max_lr  is None:\n",
        "            self.max_lr= training_context['base_lr']\n",
        "\n",
        "        self.T_current=training_context['steps']% (2 * self.T_max)\n",
        "\n",
        "        if self.T_current == 0:\n",
        "            training_context['optimizer'].adjust_learning_rate(self.max_lr,verbose=False)\n",
        "        elif (self.T_current - 1 - self.T_max) % (2 * self.T_max) == 0:\n",
        "            self.max_lr=self.max_lr*self.cosine_weight\n",
        "            training_context['optimizer'].adjust_learning_rate((self.min_lr+ (self.max_lr - self.min_lr)) *(1 - math.cos(math.pi / self.T_max)) / 2,verbose=False)\n",
        "        else:\n",
        "            training_context['optimizer'].adjust_learning_rate(\n",
        "                ((1 + math.cos(math.pi * (self.T_current / self.T_max))) * (self.max_lr - self.min_lr) * 0.5 + self.min_lr),\n",
        "                verbose=False)\n",
        "\n",
        "\n",
        "def cosine_lr( max_lr=None, min_lr=1e-7,period=1000,cosine_weight=0.2,**kwargs):\n",
        "   return CosineLR(max_lr=max_lr, min_lr=min_lr,period=period,cosine_weight=cosine_weight,**kwargs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo9BxXClwkQV"
      },
      "source": [
        "接著就來執行一下，看看機器讀完上萬首唐詩會出現甚麼樣的作品。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgpMM16ODHWT"
      },
      "source": [
        "\n",
        "def SequenceCrossEntropyLoss2(output, target):\n",
        "    target = target.reshape(-1)\n",
        "    mask = (target>4)*(target!=data_provider.text2index(' ')).bool()\n",
        "    masked_target = target[mask]\n",
        "    masked_output = output[mask, :]\n",
        "    return 0.5 * CrossEntropyLoss(reduction='mean', axis=-1)(masked_output, masked_target) + CrossEntropyLoss(reduction='mean', auto_balance=True, axis=-1)(output, target)\n",
        "\n",
        "\n",
        "def SequenceAccuracy2(output, target):\n",
        "    target = target.reshape(-1)\n",
        "    mask = (target>4)*(target!=data_provider.text2index(' ')).bool()\n",
        "    masked_target = target[mask]\n",
        "    masked_output = output[mask, :]\n",
        "    return accuracy(masked_output, masked_target, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TPWq8hzwkQV"
      },
      "source": [
        "\n",
        "lstm_poet2\\\n",
        "    .with_optimizer(optimizer='Adam',lr=1e-3,betas=(0.9, 0.999))\\\n",
        "    .with_loss(SequenceCrossEntropyLoss2)\\\n",
        "    .with_metric(SequenceAccuracy2,name='accuracy')\\\n",
        "    .with_regularizer('l2')\\\n",
        "    .with_model_save_path('Models/{0}.pth.tar'.format(item))\\\n",
        "    .trigger_when('on_batch_end',action=write_something,frequency=300,unit='batch') \\\n",
        "    .with_learning_rate_scheduler(cosine_lr(max_lr=1e-3,min_lr=1e-7,period=1000,cosine_weight=0.2)) \\\n",
        "    .with_automatic_mixed_precision_training()\n",
        "\n",
        "    #.with_optimizer(optimizer='Adam',lr=1e-3,betas=(0.9, 0.999))\\ 設定優化器\n",
        "    #.with_loss(SequenceCrossEntropyLoss)\\ 設定損失函數\n",
        "    #.with_metric(SequenceAccuracy,name='accuracy')\\ 設定評估函數\n",
        "    #.with_regularizer('l2')\\ 設定l2regularizer\n",
        "    #.with_model_save_path('Models/{0}.pth.tar'.format(item))\\ 設定模型存檔路徑\n",
        "    #.trigger_when('on_batch_end', action=write_something) \\ #設定觸發任務write_something\n",
        "    #設定調整學習速率時程\n",
        "    #.with_learning_rate_scheduler(reduce_lr_on_plateau,monitor='accuracy',mode='max',factor=0.5,patience=3,cooldown=0,threshold=5e-4,warmup=0) \\\n",
        "    #.with_automatic_mixed_precision_training() #設定為自動混合精度訓練\n",
        "    \n",
        "\n",
        "plan2=TrainingPlan()\\\n",
        "    .add_training_item(lstm_poet2)\\\n",
        "    .with_data_loader(data_provider)\\\n",
        "    .repeat_epochs(30)\\\n",
        "    .with_batch_size(32)\\\n",
        "    .print_progress_scheduling(20,unit='batch')\\\n",
        "    .display_loss_metric_curve_scheduling(frequency=200,unit='batch',imshow=True)\\\n",
        "    .print_gradients_scheduling(100)\\\n",
        "    .save_model_scheduling(50,unit='batch')\n",
        "\n",
        "    #.add_training_item(lstm_poet2)\\ 加入要訓練的模型\n",
        "    #.with_data_loader(data_provider)\\ 設定資料提供者\n",
        "    #.repeat_epochs(50)\\  設定總epoch數\n",
        "    #.with_batch_size(32)\\ 設定批次大小\n",
        "    #.print_progress_scheduling(20,unit='batch')\\ 設定列印訓練狀態的時程\n",
        "    #.display_loss_metric_curve_scheduling(frequency=200,unit='batch',imshow=True)\\ 設定顯示loss-metrics時間走勢圖的時程\n",
        "    #.print_gradients_scheduling(200)\\ 設定列印梯度狀況的時程\n",
        "    #.save_model_scheduling(50,unit='batch') 設定存檔的時程\n",
        "\n",
        "\n",
        "plan2.start_now()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itgHftrzwkQW"
      },
      "source": [
        "#寫藏頭詩\n",
        "def get_hidden_poetry(heading,temperature = 1.8):\n",
        "\n",
        "    model =load('Models/{0}.pth.tar'.format(item))\n",
        "    # 模型千萬記得要轉換成eval模式\n",
        "    model.eval()\n",
        "    for  module in model.modules():\n",
        "        if isinstance(module, LSTM):\n",
        "            module.stateful=True\n",
        "    #清除模型狀態\n",
        "#     for module in model.modules():\n",
        "#         if isinstance(module, LSTM):\n",
        "#             module.clear_state()\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "\n",
        "    print()\n",
        "    print('----- 以「{0}」寫藏頭詩'.format(heading))\n",
        "\n",
        "    print('----- temperature:', temperature)\n",
        "\n",
        "    \n",
        "    text_generated = []\n",
        "    heading_idx=0\n",
        "    start_string = heading[heading_idx]\n",
        "    text_generated.append('[CLS]')\n",
        "    text_generated.append(heading[heading_idx])\n",
        "    \n",
        "    heading_idx+=1\n",
        "\n",
        "\n",
        "    seq = [data_provider.text2index(s) for s in start_string]\n",
        "    seq.insert(0, 0)\n",
        "    # seq.append(1)\n",
        "    input_eval = to_tensor([seq]).long().detach()\n",
        "    if ndim(input_eval)<2:\n",
        "        input_eval=input_eval.expand_dims(0)\n",
        "\n",
        "\n",
        "    is_finished = False\n",
        "    num_generate = 0\n",
        "    row_length=0\n",
        "\n",
        "    sys.stdout.write(start_string)\n",
        "\n",
        "    while not is_finished:\n",
        "        try:\n",
        "\n",
        "            predictions = model(input_eval)[-1]\n",
        "            #將溫度設定僅限於機率最高的十個字\n",
        "            predicted_idx=argsort(predictions)[:10]\n",
        "            predicted_probs =  clip(predictions[predicted_idx],1e-8,1-1e-8)\n",
        "\n",
        "            predicted_id = predicted_idx[multinomial(predicted_probs/ temperature, num_samples=1)].item()\n",
        "            \n",
        "            input_eval = to_tensor([[predicted_id]]).long().detach()\n",
        "            \n",
        "\n",
        "            if text_generated[-1]==' ' and heading_idx<len(heading):\n",
        "                text_generated.append(heading[heading_idx])\n",
        "                #更新字頭 覆寫成為下次的輸入\n",
        "                input_eval = to_tensor([[data_provider.text2index(heading[heading_idx])]]).long().detach()\n",
        "                heading_idx+=1\n",
        "            else:\n",
        "                text_generated.append(data_provider.index2text(predicted_id))\n",
        "            \n",
        "            \n",
        "            if ndim(input_eval) < 2:\n",
        "                input_eval = input_eval.expand_dims(0)\n",
        "                \n",
        "            \n",
        "            if len(text_generated)>10 and len(list(set(text_generated[-5:])))==1:\n",
        "                is_finished = True\n",
        "\n",
        "                break\n",
        "            if text_generated[-2] == '[PAD]' and text_generated[-1] == '[PAD]':\n",
        "                sys.stdout.write('\\n\\n')\n",
        "                sys.stdout.flush()\n",
        "                is_finished = True\n",
        "            if text_generated[-1] == '[SEP]' :\n",
        "                sys.stdout.write('\\n')\n",
        "                sys.stdout.flush()\n",
        "            elif text_generated[-1] not in ['[CLS]','[PAD]','[UNK]']:\n",
        "                sys.stdout.write(text_generated[-1])\n",
        "                row_length += 1\n",
        "                if row_length >= 47 and text_generated[-1] in ['，', '。',' ', '[SEP]']:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    is_finished=True\n",
        "                sys.stdout.flush()\n",
        "            num_generate += 1\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    model.train()\n",
        "    print()\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KblNyOwkQY"
      },
      "source": [
        "get_hidden_poetry('白日依山黃河入海')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QyBc2jbwkQY"
      },
      "source": [
        "get_hidden_poetry('沉魚落雁閉月羞花',1.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lf2Y6hCwkQZ"
      },
      "source": [
        "get_hidden_poetry('疫情當頭台灣加油')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}